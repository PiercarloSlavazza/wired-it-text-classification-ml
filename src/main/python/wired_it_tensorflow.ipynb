{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "embedding_dim = 200\n",
    "corpus_base_dir = os.path.join('..', '..', '..', 'corpora', 'wired_it_20190821')\n",
    "\n",
    "vocab_size = 10000\n",
    "max_length = 1000\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "num_epochs = 30\n",
    "\n",
    "training_dir = os.path.join(corpus_base_dir, 'training')\n",
    "test_dir = os.path.join(corpus_base_dir, 'test')\n",
    "classes = ['attualit_','attualit__ambiente','attualit__media','attualit__politica','attualit__tech','economia_business','economia_finanza','economia_lavoro','economia_startup','gadget_accessori','gadget_audio_e_tv','gadget_computer','gadget_elettrodomestici','gadget_foto_e_video','gadget_motori','gadget_outdoor','gadget_videogiochi','internet_regole','internet_social_network','internet_tlc','internet_web','lifestyle_design','lifestyle_food','lifestyle_mobilit_','lifestyle_salute','lifestyle_viaggi','lol','mobile_app','mobile_smartphone','mobile_tablet','play_cinema','play_cultura','play_fumetti','play_libri','play_musica','play_tv','scienza','scienza_biotech','scienza_ecologia','scienza_lab','scienza_medicina','scienza_spazio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_to_text(file_path):\n",
    "  with open(file_path, 'r', encoding=\"utf8\") as file:\n",
    "    return file.read().replace('\\n', ' ')\n",
    "def list_text_files(folder):\n",
    "  return [os.path.join(folder, file_name) for file_name in os.listdir(folder) if file_name.endswith('.txt')]\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_class_text_pairs = lambda folder: map(lambda classLabel: map(lambda file_name: (classLabel, read_file_to_text(file_name)), list_text_files(os.path.join(folder, classLabel))) , classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_classes, training_texts = zip(*flatten(list(load_class_text_pairs(training_dir))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classes, test_texts = zip(*flatten(list(load_class_text_pairs(test_dir))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from functools import reduce\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras \n",
    "from keras.models import Sequential, Model \n",
    "from keras import layers\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Input, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(training_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sequences = tokenizer.texts_to_sequences(training_texts)\n",
    "training_padded = pad_sequences(training_sequences, padding=padding_type, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "test_padded = pad_sequences(test_sequences, padding=padding_type, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove the '_' because class names features '_' as separators (and they need NOT to be split, thay must be a unique token)\n",
    "classes_tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n",
    "classes_tokenizer.fit_on_texts(classes)\n",
    "\n",
    "training_classes_seq = np.array(classes_tokenizer.texts_to_sequences(training_classes))\n",
    "test_classes_seq = np.array(classes_tokenizer.texts_to_sequences(test_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attualit_': 1, 'attualit__ambiente': 2, 'attualit__media': 3, 'attualit__politica': 4, 'attualit__tech': 5, 'economia_business': 6, 'economia_finanza': 7, 'economia_lavoro': 8, 'economia_startup': 9, 'gadget_accessori': 10, 'gadget_audio_e_tv': 11, 'gadget_computer': 12, 'gadget_elettrodomestici': 13, 'gadget_foto_e_video': 14, 'gadget_motori': 15, 'gadget_outdoor': 16, 'gadget_videogiochi': 17, 'internet_regole': 18, 'internet_social_network': 19, 'internet_tlc': 20, 'internet_web': 21, 'lifestyle_design': 22, 'lifestyle_food': 23, 'lifestyle_mobilit_': 24, 'lifestyle_salute': 25, 'lifestyle_viaggi': 26, 'lol': 27, 'mobile_app': 28, 'mobile_smartphone': 29, 'mobile_tablet': 30, 'play_cinema': 31, 'play_cultura': 32, 'play_fumetti': 33, 'play_libri': 34, 'play_musica': 35, 'play_tv': 36, 'scienza': 37, 'scienza_biotech': 38, 'scienza_ecologia': 39, 'scienza_lab': 40, 'scienza_medicina': 41, 'scienza_spazio': 42}\n"
     ]
    }
   ],
   "source": [
    "training_classes_encoded = [class_encoded[0] for class_encoded in training_classes_seq]\n",
    "classes_word_index = classes_tokenizer.word_index\n",
    "print(classes_word_index)\n",
    "classes_by_index = dict([(index, key) for (key, index) in classes_word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = []\n",
    "filter_sizes = [3,4,5]\n",
    "\n",
    "embedding_layer = keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length)\n",
    "sequence_input = Input(shape=(max_length,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "for filter_size in filter_sizes:\n",
    "    l_conv = keras.layers.Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "    l_pool = keras.layers.MaxPooling1D(pool_size=3)(l_conv)\n",
    "    convs.append(l_pool)\n",
    "\n",
    "l_merge = concatenate([convs[0],convs[1],convs[2]],axis=1)\n",
    "\n",
    "# add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "conv = keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "pool = keras.layers.MaxPooling1D(pool_size=3)(conv)\n",
    "\n",
    "x = keras.layers.Dropout(0.5)(l_merge)  \n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(128, activation='relu')(x)\n",
    "preds = keras.layers.Dense(len(classes) + 1, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 1000, 200)    2000000     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 998, 128)     76928       embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 997, 128)     102528      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 996, 128)     128128      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 332, 128)     0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 332, 128)     0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 332, 128)     0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 996, 128)     0           max_pooling1d_12[0][0]           \n",
      "                                                                 max_pooling1d_13[0][0]           \n",
      "                                                                 max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 996, 128)     0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 127488)       0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          16318592    flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 43)           5547        dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 18,631,723\n",
      "Trainable params: 18,631,723\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=[tf.keras.metrics.Precision()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13351 samples, validate on 5709 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "2 root error(s) found.\n  (0) Invalid argument: Incompatible shapes: [1,1376] vs. [1,32]\n\t [[{{node metrics_5/precision/LogicalAnd_1}}]]\n\t [[metrics_5/precision/Mean/_377]]\n  (1) Invalid argument: Incompatible shapes: [1,1376] vs. [1,32]\n\t [[{{node metrics_5/precision/LogicalAnd_1}}]]\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-d05be8ec946b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_classes_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_classes_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\tools\\Anaconda3\\envs\\tensorflow_gpu_keras\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\envs\\tensorflow_gpu_keras\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\envs\\tensorflow_gpu_keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\envs\\tensorflow_gpu_keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\envs\\tensorflow_gpu_keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument: Incompatible shapes: [1,1376] vs. [1,32]\n\t [[{{node metrics_5/precision/LogicalAnd_1}}]]\n\t [[metrics_5/precision/Mean/_377]]\n  (1) Invalid argument: Incompatible shapes: [1,1376] vs. [1,32]\n\t [[{{node metrics_5/precision/LogicalAnd_1}}]]\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "history = model.fit(training_padded, training_classes_seq, epochs=num_epochs, validation_data=(test_padded, test_classes_seq), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5709/5709 [==============================] - 2s 430us/step\n",
      "|Class Label                            |Pre |Rec |F1\n",
      "|--- |--- |--- |---\n",
      "|attualit_                               |0.15|0.14|0.15\n",
      "|attualit__ambiente                      |0.38|0.34|0.36\n",
      "|attualit__media                         |0.41|0.21|0.28\n",
      "|attualit__politica                      |0.31|0.49|0.38\n",
      "|attualit__tech                          |0.09|0.12|0.11\n",
      "|economia_business                       |0.22|0.23|0.22\n",
      "|economia_finanza                        |0.57|0.55|0.56\n",
      "|economia_lavoro                         |0.54|0.40|0.46\n",
      "|economia_startup                        |0.62|0.52|0.57\n",
      "|gadget_accessori                        |0.23|0.10|0.14\n",
      "|gadget_audio_e_tv                       |0.58|0.62|0.60\n",
      "|gadget_computer                         |0.61|0.33|0.43\n",
      "|gadget_elettrodomestici                 |0.37|0.18|0.24\n",
      "|gadget_foto_e_video                     |0.63|0.58|0.61\n",
      "|gadget_motori                           |0.62|0.73|0.67\n",
      "|gadget_outdoor                          |0.35|0.40|0.38\n",
      "|gadget_videogiochi                      |0.76|0.73|0.74\n",
      "|internet_regole                         |0.30|0.39|0.34\n",
      "|internet_social_network                 |0.53|0.53|0.53\n",
      "|internet_tlc                            |0.30|0.38|0.34\n",
      "|internet_web                            |0.28|0.28|0.28\n",
      "|lifestyle_design                        |0.24|0.15|0.18\n",
      "|lifestyle_food                          |0.59|0.45|0.51\n",
      "|lifestyle_mobilit_                      |0.66|0.42|0.51\n",
      "|lifestyle_salute                        |0.16|0.18|0.17\n",
      "|lifestyle_viaggi                        |0.28|0.35|0.31\n",
      "|lol                                     |0.42|0.41|0.42\n",
      "|mobile_app                              |0.59|0.42|0.49\n",
      "|mobile_smartphone                       |0.65|0.71|0.68\n",
      "|mobile_tablet                           |0.60|0.61|0.61\n",
      "|play_cinema                             |0.50|0.71|0.59\n",
      "|play_cultura                            |0.12|0.27|0.16\n",
      "|play_fumetti                            |0.73|0.51|0.60\n",
      "|play_libri                              |0.66|0.59|0.62\n",
      "|play_musica                             |0.65|0.54|0.59\n",
      "|play_tv                                 |0.59|0.58|0.58\n",
      "|scienza                                 |0.16|0.12|0.14\n",
      "|scienza_biotech                         |0.20|0.28|0.23\n",
      "|scienza_ecologia                        |0.40|0.47|0.44\n",
      "|scienza_lab                             |0.28|0.32|0.30\n",
      "|scienza_medicina                        |0.36|0.49|0.42\n",
      "|scienza_spazio                          |0.73|0.76|0.75\n",
      "\n",
      "|Average Type |Prec |Rec |F1\n",
      "|--- |--- |--- |---\n",
      "|micro|0.42|0.42|0.42\n",
      "|macro|0.44|0.42|0.42\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "classes_probabilties = model.predict(test_padded, batch_size=32, verbose=1)\n",
    "predicted = np.argmax(classes_probabilties, axis=1).tolist()\n",
    "expected = flatten(test_classes_seq)\n",
    "precision_scores = metrics.precision_score(expected, predicted, labels=np.unique(expected), average=None)\n",
    "recall_scores = metrics.recall_score(expected, predicted, labels=np.unique(expected), average=None)\n",
    "f1_scores = metrics.f1_score(expected, predicted, labels=np.unique(expected), average=None)\n",
    "precion_recall_f1_scores = list(zip(precision_scores, recall_scores, f1_scores))\n",
    "print('|Class Label'.ljust(40, ' ') + '|Pre |Rec |F1')\n",
    "print('|--- |--- |--- |---')\n",
    "for index, precision_recall_f1 in enumerate(precion_recall_f1_scores):\n",
    "    classLabel = classes_by_index[index + 1]\n",
    "    print('|%s|%.2f|%.2f|%.2f' % (classLabel.ljust(40, ' '), precision_recall_f1[0], precision_recall_f1[1], precision_recall_f1[2]))\n",
    "\n",
    "precision_score_micro_averaged = metrics.precision_score(expected, predicted, labels=np.unique(expected), average='micro')\n",
    "precision_score_macro_averaged = metrics.precision_score(expected, predicted, labels=np.unique(expected), average='macro')\n",
    "\n",
    "recall_score_micro_averaged = metrics.recall_score(expected, predicted, labels=np.unique(expected), average='micro')\n",
    "recall_score_macro_averaged = metrics.recall_score(expected, predicted, labels=np.unique(expected), average='macro')\n",
    "\n",
    "f1_score_micro_averaged = metrics.f1_score(expected, predicted, labels=np.unique(expected), average='micro')\n",
    "f1_score_macro_averaged = metrics.f1_score(expected, predicted, labels=np.unique(expected), average='macro')\n",
    "\n",
    "print()\n",
    "print('|Average Type |Prec |Rec |F1')\n",
    "print('|--- |--- |--- |---')\n",
    "print('|micro|%.2f|%.2f|%.2f' % (precision_score_micro_averaged, recall_score_micro_averaged, f1_score_micro_averaged))\n",
    "print('|macro|%.2f|%.2f|%.2f' % (precision_score_macro_averaged, recall_score_macro_averaged, f1_score_macro_averaged))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
